WEBVTT

1
00:01:17.880 --> 00:01:19.579
Sadia Shaheed: Hi, Linda, how are you?

2
00:01:56.010 --> 00:02:15.130
Brian Beatty: Okay, Welcome. This is session, too. By take 8, 30, I'm. I'm laughing because we've been talking about the discussion for for the last 20 min. Sorry I didn't get that on the recording, but now we're on the recording, and we're talking now about maybe changing the schedule, so that we alleviate the overlap between itech and 40 assignments, and I take a 30 assignments.

3
00:02:15.490 --> 00:02:19.760
Brian Beatty: And so the proposal I'm. Making is that we push everything down a week. In other words.

4
00:02:19.910 --> 00:02:23.450
the first paper in this class wouldn't be due until session

5
00:02:23.500 --> 00:02:25.279
session 6 essentially.

6
00:02:25.440 --> 00:02:28.829
Brian Beatty: and then it would be due again. The next one would be due in session

7
00:02:29.060 --> 00:02:36.619
Brian Beatty: 10, and the next session would be, it would be do and be session a 13,

8
00:02:36.660 --> 00:02:38.330
Brian Beatty: and then we would have

9
00:02:39.180 --> 00:02:41.010
Brian Beatty: we would probably

10
00:02:41.110 --> 00:02:47.420
Brian Beatty: I i'm thinking that it might be better to push the final presentations back a week to just so. There's a little bit more time there.

11
00:02:47.850 --> 00:02:53.620
Stephen Cadette: I I think if you were to push. I'm sorry. Did you say Push

12
00:02:53.950 --> 00:02:57.490
Stephen Cadette: the one from the week of the third to the week of the tenth in April.

13
00:02:58.120 --> 00:02:58.870
Brian Beatty: No.

14
00:02:59.430 --> 00:03:03.599
Stephen Cadette: I'm not. I'm not talking about dates. I'm talking about session numbers.

15
00:03:04.080 --> 00:03:07.820
Stephen Cadette: So yeah, I were you saying, push the ninth to the tenth.

16
00:03:08.870 --> 00:03:09.650
Yeah.

17
00:03:09.700 --> 00:03:18.590
Brian Beatty: So I me, am I? Am I sharing my screen? Yes, I am right. So that means right now. The first paper is due on march first.

18
00:03:19.310 --> 00:03:25.890
Brian Beatty: Okay, so that that means is that the first paper would be due on March Eighth. I believe that's right. I think that's the right date.

19
00:03:27.210 --> 00:03:28.369
Brian Beatty: Just if that's right.

20
00:03:28.430 --> 00:03:29.400
Brian Beatty: Yeah.

21
00:03:30.760 --> 00:03:33.329
Brian Beatty: all right. So that means that means

22
00:03:33.450 --> 00:03:36.800
Brian Beatty: Session 6. Essentially. We'd be doing our first presentations.

23
00:03:37.760 --> 00:03:49.110
Brian Beatty: Okay, and then that means and then we everything moves down a week. So for spring break that doesn't move, then Session 8, which is right now. The first papers doing that second papers on April Fifth. That would become April twelfth.

24
00:03:50.440 --> 00:03:58.679
Brian Beatty: Okay, and then the last small paper which right now is, is required. April 26 would actually be due May. Third.

25
00:03:59.940 --> 00:04:09.770
Brian Beatty: okay? And then, if we continue this down the road, then what what probably would be better is if the draft of the presentations in May would be due on the seventeenth instead of the tenth.

26
00:04:12.980 --> 00:04:18.150
Stephen Cadette: Okay, that's really what that that makes that big of a deal. But

27
00:04:18.519 --> 00:04:23.509
Stephen Cadette: if we move the week of the night to the tenth. That'll overlap with 842.

28
00:04:23.970 --> 00:04:32.809
Stephen Cadette: Not that that matters because it's going to overlap you the way I think.

29
00:04:32.860 --> 00:04:38.060
Brian Beatty: and so that's a little easier to adjust. As a matter of fact, we might end up adjusting a week in 842. Anyway.

30
00:04:38.530 --> 00:04:44.409
Brian Beatty: But let me write that down as a note. So if we do this, yeah, look at 842 over that.

31
00:04:46.940 --> 00:05:00.430
Brian Beatty: Yeah, we can't we can't get rid of everything. But we might. Yeah, it might make sense, because but honestly, that one is got the next couple of a week or 2 extra in that, anyway, because it's it's a there's so many social aspects that we're going to talk about in that class.

32
00:05:01.500 --> 00:05:07.849
Brian Beatty: so it might be enough to move that up a week, and that gives us a little more time for the final project. Maybe that'd be a good idea, anyway.

33
00:05:09.970 --> 00:05:10.690
Brian Beatty: Okay.

34
00:05:16.340 --> 00:05:21.229
Brian Beatty: since you're here live, you get to help make this decision, because I want to make the decision tonight because I have to do a lot of

35
00:05:21.400 --> 00:05:23.770
Brian Beatty: rearranging before next week if we're going to do that.

36
00:05:24.120 --> 00:05:35.299
Brian Beatty: So. So so let's for now. Let's say, okay, we wanna we're gonna move the move them weeks down. And i'll by early next week, probably by before the end of the weekend. I'll have all that adjusted

37
00:05:35.440 --> 00:05:41.300
Brian Beatty: if you have a. If you have a problem with that, you don't want to say anything about it. Now, which you know that's fine. Send me a note.

38
00:05:41.350 --> 00:05:59.049
Brian Beatty: Let me know. We can talk online, or you can just talk to me through the network through the email or the the messaging. And and you know I don't want to make. I don't want to make a change like this if it's going to be a significant issue for anyone. And there's also a way, maybe, of doing something more flexibly. But that's it gets more complicated.

39
00:05:59.410 --> 00:06:09.600
Brian Beatty: Okay. So then the second question is, Would you value a whole whole module a whole week, you know, kind of exploring and doing some activities.

40
00:06:09.660 --> 00:06:10.790
Brian Beatty: and

41
00:06:11.750 --> 00:06:12.850
Brian Beatty: and then

42
00:06:14.270 --> 00:06:16.370
Brian Beatty: activities and

43
00:06:16.480 --> 00:06:20.570
discussion around the use of AI and education

44
00:06:21.090 --> 00:06:25.430
Brian Beatty: as an example of a emerging technology. That is kind of taking

45
00:06:25.580 --> 00:06:30.160
Brian Beatty: taking the high right world by storm. At least we start a lot of our conversations.

46
00:06:32.640 --> 00:06:35.720
Maristella Tapia: Dr. B. I I think that would be really cool.

47
00:06:36.010 --> 00:06:47.100
Maristella Tapia: And I would also suggest, maybe something on like assistive technology at some point, if we can talk about that in any way, shape or form. I'd like to learn more about

48
00:06:47.320 --> 00:07:10.119
Maristella Tapia: at and any emerging technology which you know AI might be an example of a kind of at in some ways, but my students and writing all their essays for them. But yeah it I I just wanted to know if there might be any space, and maybe it's not in this class. Maybe it's somewhere else in the program. But

49
00:07:10.400 --> 00:07:12.270
Maristella Tapia: for you know.

50
00:07:12.610 --> 00:07:28.269
Brian Beatty: So here's what I might do with that. And and this is it's a it's a good suggestion, I think, what it what maybe, could that could be. Is this the session? 13, which is kind of something that didn't it really didn't fit anywhere?

51
00:07:29.540 --> 00:07:43.610
Brian Beatty: I just wanted to make sure we were talking about it. This could be a place where where there's a couple of tracks here and assistive technology could be one of the tracks that week, right where there's some reasons you can pick, you know you can, you know, if you're interested more in personal learning environments

52
00:07:43.620 --> 00:07:51.700
Brian Beatty: to you know. Do these kind of preparations? And then with the the live class i'd we'd have to orchestrate so that we may be doing something that we all can get some benefit from.

53
00:07:51.990 --> 00:08:01.889
Brian Beatty: You know. Maybe we're doing some summarizing, and everybody can hear some of the summaries of what other people have found out about these, because it's important to know about assistive technologies, Even if you're not using them in your classes.

54
00:08:02.440 --> 00:08:03.120
Brian Beatty: Right

55
00:08:03.240 --> 00:08:07.319
Brian Beatty: because they they are, they are. They are important.

56
00:08:08.450 --> 00:08:12.929
Brian Beatty: and I think in many places they would be a considered emerging, because most of us

57
00:08:12.960 --> 00:08:18.899
Brian Beatty: may have. We may have no idea what the system technologies really are, or do, or what it what the capabilities are.

58
00:08:22.030 --> 00:08:25.320
Brian Beatty: so i'll i'll. I'll take a look at that. I have it written down here.

59
00:08:25.740 --> 00:08:28.950
Brian Beatty: and but I think that probably would be where I wouldn't

60
00:08:29.720 --> 00:08:30.750
Brian Beatty: in there.

61
00:08:34.090 --> 00:08:34.799
Brian Beatty: Yeah.

62
00:08:34.870 --> 00:08:38.620
Brian Beatty: I mean, I use the system technologies. I just don't call them assistive technologies.

63
00:08:38.770 --> 00:08:46.730
Brian Beatty: You know that that that that live captioning of the the audio stream that my computer is is going through the that's an assistive technology

64
00:08:48.040 --> 00:08:58.030
Brian Beatty: alright, My use of grammarly, you know, to help me my misspellings. That's an assistant technology.

65
00:08:58.570 --> 00:09:05.379
Brian Beatty: I mean, I I don't. I don't do touch typing the way i'm supposed to, and at least a lot of a lot of mistakes. I guess

66
00:09:06.510 --> 00:09:07.370
Brian Beatty: so.

67
00:09:07.870 --> 00:09:08.640
Brian Beatty: Okay.

68
00:09:09.730 --> 00:09:17.550
Brian Beatty: all right. So what I hear what I think I've heard was that an AI topic would be valued. Yes, yes, I see a lot of yeses. I think that was for that question.

69
00:09:17.690 --> 00:09:18.500
Brian Beatty: right?

70
00:09:19.130 --> 00:09:34.810
Brian Beatty: All right. So then they say one. So. Another decision to make. That is what I where i'd like to see that go, then is next week. Next week is the asynchronous only week. I'm not available for the synchronous meeting next week, and so I think I think that's one that that is

71
00:09:34.820 --> 00:09:47.369
Brian Beatty: is set up well for us to explore either individually or and you could work with other people or things like that, and then we could do more. We could do a lot of asynchronous exchange of information, some reason things that we could do a debrief on that

72
00:09:47.380 --> 00:09:55.580
Brian Beatty: at the start of session. What current session 3, which is where we start talking about content because content is a big part of what AI is helping us with right now.

73
00:09:56.450 --> 00:09:59.509
Brian Beatty: at least, and there are things that many of us are playing around with.

74
00:10:03.140 --> 00:10:04.030
Brian Beatty: So

75
00:10:04.380 --> 00:10:11.400
Brian Beatty: if we do it next week, and we do it asynchronously. Here's what I've Here's what my plan would be. I was working on this earlier today.

76
00:10:13.210 --> 00:10:14.519
Brian Beatty: It's not that one.

77
00:10:15.190 --> 00:10:16.159
Brian Beatty: It's this one.

78
00:10:16.760 --> 00:10:25.380
Brian Beatty: So what I what I what I have here is a Google document that I think this this is what I was thinking about, and you can let me know if you

79
00:10:25.800 --> 00:10:40.480
Brian Beatty: you, you could probably help me refine this or scrap it if it's like, you know it doesn't sound like I want to do that. Basically AI and education. We're exploring this emerging technology together. And the idea here is that you would pick an area to explore

80
00:10:40.690 --> 00:10:51.800
Brian Beatty: during the week. Basically i'm not going to give you necessarily a set of readings. I have a link here to a padlet. That's a collection that one of our you know kind of an informal leader in in the area

81
00:10:52.250 --> 00:11:10.470
Brian Beatty: has been called kind of collaborating these. Someone who works does a lot of social media stuff in in tech at the higher level at least what links to websites, blog posts, articles, you know, different tools and other kinds of things, including other people's, summaries, and you would find it, find a topic you to explore it.

82
00:11:10.480 --> 00:11:26.729
Brian Beatty: and then you would give us a summary of it. Right? So you would. You would figure out a topic we could brainstorm some of these, or you could just come up with your own, or ask others, or ask me if you they couldn't find out whether and you put it put you start a new section. You put your name there, and then you would

83
00:11:26.740 --> 00:11:46.019
Brian Beatty: put a summary in there. Not not a lot, 250 words or less, something like that. And then you would allow other others would read this, and then we would have a a an ongoing discussion and asynchronous discussion, you know, you know, over the next. Really, I guess the the week that that week

84
00:11:46.140 --> 00:12:03.749
Brian Beatty: the preparation would be do some exploration instead of readings to exploration. Then, during the week you you, you put your summary in here, and then we're doing the online discussion, and i'll have a prompt. That kind of takes you back into this as the the source data. And then we talk about it in various places.

85
00:12:04.970 --> 00:12:22.699
Brian Beatty: I just picked up this first one, I said. Well, let's start with what is AI? What is artificial intelligence? I thought, Well, why not ask AI what AI is? And so I I I decided to play around with. Well, how would I cite this? Well, here's the tool I use the link to the tool, and here's the prompt I use, and 250 words or less. What is artificial intelligence?

86
00:12:22.710 --> 00:12:31.760
Brian Beatty: And I put it in here, and I said you, you could do this, as a matter of fact, in your own exploration, to find a summary. But if you do that, then I would want you to also put your own critique in there of the summary.

87
00:12:31.870 --> 00:12:42.720
Brian Beatty: so you can't just rely completely on the AI engine to give you the answers you're looking for. But you have to do some of your own work on it, so that you could do some sort of critique on it. I just did a real simple thing.

88
00:12:42.800 --> 00:12:49.290
Brian Beatty: I thought. And I basically said, Well, you know a good general explanation, you know, kind of especially from a a bit of a computer science.

89
00:12:49.320 --> 00:13:07.439
Brian Beatty: a perspective without getting too technical. Yet it says nothing about the use of the applications in education. So for us it's wouldn't be. This wouldn't be enough. This wouldn't be sufficient for us, because we would want to hear a little bit more about. Well, how is it using education? How has it been using education? What are the implications and those kind of things.

90
00:13:07.450 --> 00:13:12.699
Brian Beatty: And then you would just basically put your topic in here, and then you would put your summary in there, you know. Obviously

91
00:13:12.850 --> 00:13:16.899
Brian Beatty: we use more space, and then by the so, by the end of the week we'd have

92
00:13:17.160 --> 00:13:20.590
Brian Beatty: How many people are in the class participating in 1314.

93
00:13:20.610 --> 00:13:22.090
Brian Beatty: Excuse me, something like that.

94
00:13:22.470 --> 00:13:34.429
Brian Beatty: We'd have 13 or 14 those summary articles, and this would be then our source document for the week that then we would kind of you know. Then that would be something that we would use for. Our Our

95
00:13:35.670 --> 00:13:37.880
Brian Beatty: discussions would kind of come off. Of this.

96
00:13:38.730 --> 00:13:40.870
Brian Beatty: I still have to come up with a prompt. But

97
00:13:42.130 --> 00:13:45.599
Brian Beatty: that's that's what I was. That's what I came up with. That was my

98
00:13:46.690 --> 00:13:49.019
Brian Beatty: initial design thinking on this.

99
00:13:50.550 --> 00:13:58.979
Brian Beatty: So we're basically crowdsourcing the content. And in some ways, maybe even AI sourcing content around AI depend upon how much use tools like that.

100
00:14:00.370 --> 00:14:10.660
Adam Hill (he/him/his): And so, as a result next week would be the AI. We would be you an Async.

101
00:14:10.710 --> 00:14:13.229
Brian Beatty: introducing the topic of data visualization.

102
00:14:13.540 --> 00:14:17.659
Brian Beatty: maybe a little bit more comp maybe better to do with the synchronous meeting

103
00:14:17.850 --> 00:14:37.689
Brian Beatty: then just to do it asynchronous. Only I think it might be more helpful for, for especially if you're not used to thinking about? What is the age of the visualization, and how can we use it, etc.? So you know, i'm more comfortable with this topic as asynchronous than the beginning of data visualization. Quite honestly.

104
00:14:45.910 --> 00:14:47.499
Brian Beatty: any any comments.

105
00:14:49.010 --> 00:15:04.980
Adam Hill (he/him/his): I guess my other question would be so. When, when, when we would send, we, we need to submit our thing by a certain time, and then for us to also review it. And so we're kind of having that asing dialogue

106
00:15:04.990 --> 00:15:10.390
Brian Beatty: 8 am. Thursday next week to have your contribution to the page here.

107
00:15:10.660 --> 00:15:27.540
Brian Beatty: So you you got time that you may have set aside for the live class. You can use that time you can use your prep time for your readings, and but by after like, by by Thursday next week, i'd like it to be available for others to be reading, because the discussion prompt will require you to come back in here, and

108
00:15:27.550 --> 00:15:46.030
Brian Beatty: and I don't exactly sure what it' but it'll be. Draw from what people other people have written, maybe asking questions about it. All those kinds of things. That's probably what i'm gonna do is to ask you. You kind of ask questions to other people, and then look for those for responses throughout the week. So it does have to be here, but it doesn't have to be here.

109
00:15:46.040 --> 00:15:51.460
Brian Beatty: I would say, before the class starts ideally, maybe, but then i'd have to rely on you, having scheduled time to do

110
00:15:51.560 --> 00:15:59.640
Brian Beatty: all the readings, and and sometimes I know that doesn't work out so well. But you should have this time kind of scheduled out. But we weren't going to do a lot class, anyway. But

111
00:15:59.740 --> 00:16:03.290
Brian Beatty: you know, anyway, I think we have to keep some synchronicity to it.

112
00:16:03.550 --> 00:16:06.729
Brian Beatty: So that's what My, that's what my perspective is on it.

113
00:16:08.830 --> 00:16:17.269
Stephen Cadette: And Dr. Bailey, did you say data? Visualization is what had been scheduled? That's 740. Never mind.

114
00:16:17.520 --> 00:16:25.960
Brian Beatty: I got my. I got my my technology classes crossed. Yeah, the the visualizations in in the other class.

115
00:16:26.200 --> 00:16:31.259
Brian Beatty: We're going to do that, too. Yeah, we're going to do that asynchronously. Anyway.

116
00:16:31.840 --> 00:16:35.319
Brian Beatty: I just told you. I don't know that it's that great of an idea, but

117
00:16:35.660 --> 00:16:39.210
it's kind of a it's a circumstance.

118
00:16:39.650 --> 00:16:47.249
Brian Beatty: Yeah. So here we're going to talk about emerging technologies and content which essentially this is a good

119
00:16:47.330 --> 00:16:48.580
Brian Beatty: dive into

120
00:16:48.780 --> 00:16:49.910
Brian Beatty: just in general.

121
00:16:54.930 --> 00:16:58.779
Maristella Tapia: we're generating

122
00:16:58.880 --> 00:17:02.679
Maristella Tapia: having us just kind of explore different options.

123
00:17:03.040 --> 00:17:06.530
Maristella Tapia: And are you saying that you would have us

124
00:17:06.790 --> 00:17:13.420
Maristella Tapia: like, write our own prompts, or are you going to write the prompts? And then we find

125
00:17:13.470 --> 00:17:16.079
Brian Beatty: I I I would allow you to pick a topic.

126
00:17:16.630 --> 00:17:25.170
Brian Beatty: and you can come up with that topic, and you put it in here, and then you do some exploration on it, or you may do some exploration first, and figure out well what's out there?

127
00:17:25.290 --> 00:17:40.160
Brian Beatty: What's interesting to me, and then come in and do a summary of a particular topic. I'm not going to dictate the topics to you. If if you find that someone has already started a topic, or or you do some informal talking to others and say, hey, we're both interested in this. You could do these together.

128
00:17:40.660 --> 00:17:54.470
Brian Beatty: Okay, You do not have to do this completely individually. You can do this together. If you do it in. If you do it together, then I would expect you to be, you know. Maybe you know, since 2 of you are putting effort into it. Maybe it's a little more expanded. I don't know. Maybe there's a video explanation, Whatever you'd want to do

129
00:17:55.890 --> 00:17:56.899
Brian Beatty: that makes sense.

130
00:18:02.460 --> 00:18:10.519
Brian Beatty: But this is not. This is not the kind of assignment I would. I would I would require you to be in a group with, because you, haven't been really talking about that at all.

131
00:18:11.720 --> 00:18:15.409
Brian Beatty: Some assignments, yeah, you' to have to do it as a group, this one that you don't have to

132
00:18:16.060 --> 00:18:18.830
Brian Beatty: the topic related to AI, though. Yes.

133
00:18:18.950 --> 00:18:20.400
Brian Beatty: Good question, Amanda.

134
00:18:20.470 --> 00:18:21.240
Brian Beatty: Yes.

135
00:18:21.570 --> 00:18:28.379
Brian Beatty: because that's what we're doing. We're exploring the the the the overall topic is artificial intelligence and education. AI, and education.

136
00:18:29.340 --> 00:18:35.439
Brian Beatty: And if you go to this pablet, if you, if you look at this oh, actually it is it here? Yeah, here it is, right here.

137
00:18:35.650 --> 00:18:40.790
Brian Beatty: This is a heather brown who was part of the yeah edge of cause community.

138
00:18:40.860 --> 00:18:43.930
Brian Beatty: you know, higher at it, that kind of thing she's been. She's been

139
00:18:44.000 --> 00:18:48.879
Brian Beatty: capturing all of these resources that she's aware of that come to her

140
00:18:49.090 --> 00:18:53.649
and putting them in here. And some of these others are resource hubs themselves

141
00:18:54.280 --> 00:19:02.729
Brian Beatty: different generators. There's a lot of different things you can explore. So you could pick a generator and talk about one of the AI generators, either for text or imagery.

142
00:19:03.050 --> 00:19:08.859
Brian Beatty: or anything else, or maybe some other things. You know there's anyone looked at AI videos.

143
00:19:11.630 --> 00:19:14.319
Brian Beatty: There are tools out there that will do some of this stuff.

144
00:19:14.370 --> 00:19:17.470
There's an AI tool to remove the

145
00:19:18.980 --> 00:19:20.450
the

146
00:19:20.500 --> 00:19:21.860
the

147
00:19:22.160 --> 00:19:26.690
Stephen Cadette: watermark, the watermark. Yeah, You mentioned it yesterday. The watermarks from photos.

148
00:19:26.850 --> 00:19:37.230
Brian Beatty: Imagine that I mean you could talk about the ethics of a I mean, that's a tool, for example, that you know, you might say. Well, this could be a problem. Watermarks are sometimes, therefore, you know

149
00:19:37.720 --> 00:19:39.970
Brian Beatty: purposes of identity.

150
00:19:40.120 --> 00:19:40.950
Brian Beatty: Right?

151
00:19:40.970 --> 00:19:46.609
Brian Beatty: Say, hey, this is not. This is owned by this or this is provided by this company, and someone removes the watermark. Now

152
00:19:46.640 --> 00:19:52.040
Brian Beatty: you know, kind of their their claim is gone there. And so that's something we need to be concerned about.

153
00:19:53.390 --> 00:20:09.010
Brian Beatty: What about You know? Some of you may be interested more on the policy side of it. Well, what about intellectual property? What about student? You know plagiarism? What you know? What about you know? Should this be allowed, or how would you control for this, or what would you tell students is available, or or how about

154
00:20:09.230 --> 00:20:12.310
the use of it by teachers themselves

155
00:20:12.700 --> 00:20:16.109
Brian Beatty: there you can. We actually looked at today?

156
00:20:16.230 --> 00:20:25.049
Brian Beatty: We asked the AI to give us a marketing plan, for you know a a a a particular project, you know, like

157
00:20:25.180 --> 00:20:30.310
Brian Beatty: 500 words, you know a a summary of a market, and it did actually a pretty good job

158
00:20:30.610 --> 00:20:31.790
Brian Beatty: wouldn't use it.

159
00:20:31.990 --> 00:20:33.689
Brian Beatty: but it might be a good starting point.

160
00:20:33.820 --> 00:20:35.430
Brian Beatty: You know. What about that

161
00:20:37.540 --> 00:20:43.000
Brian Beatty: hypothesis? Yeah. Hypothesis that they're the the Social Annotation Company.

162
00:20:44.160 --> 00:20:56.029
Maristella Tapia: Yeah, I I've used it for some of the articles from last semester. But this is happening Tomorrow you can register. I actually just registered for it.

163
00:20:56.050 --> 00:20:59.850
Maristella Tapia: and I think it's a a conversation

164
00:20:59.990 --> 00:21:01.699
with some

165
00:21:02.160 --> 00:21:09.870
Maristella Tapia: AI scholars regarding you know, how is this impacting higher education? How are

166
00:21:10.090 --> 00:21:14.750
Maristella Tapia: teachers dealing with it? And so on and so forth?

167
00:21:15.960 --> 00:21:24.850
Brian Beatty: Yeah. So hot Topic: yeah. Everybody who's got podcasts in higher education is looking for. Okay, we need an AI segment.

168
00:21:24.870 --> 00:21:27.140
Brian Beatty: Either they've had it or they're gonna have it coming up.

169
00:21:27.400 --> 00:21:29.459
Brian Beatty: So yeah.

170
00:21:32.200 --> 00:21:33.010
Brian Beatty: Okay.

171
00:21:35.090 --> 00:21:53.379
Brian Beatty: all right. So I will. I'll flush that out a little bit more, and i'll move. I'll move the molecules around. I'll create a module for next week that it is AI in education include links in a. You know. You know the the what I expect you to do, and maybe maybe a little video video explanation of that, too.

172
00:21:55.070 --> 00:21:56.680
Brian Beatty: Yeah.

173
00:21:57.990 --> 00:22:03.249
Brian Beatty: publishing. There are. There are a bunch of online sites that use AI to write their articles. I mean, that's pretty clear.

174
00:22:07.970 --> 00:22:17.169
Brian Beatty: It seems like my local newspaper right? Must. It seems like they write articles. I mean, they're talking about stuff. They they don't have a reporter doing this. I know that.

175
00:22:18.820 --> 00:22:20.200
Brian Beatty: All right. Okay.

176
00:22:20.370 --> 00:22:21.360
Brian Beatty: Super.

177
00:22:22.730 --> 00:22:23.869
Brian Beatty: So that's that one.

178
00:22:24.170 --> 00:22:25.020
Brian Beatty: Well.

179
00:22:25.890 --> 00:22:29.210
Brian Beatty: that I thought we would have this done by 725.

180
00:22:32.280 --> 00:22:47.599
Brian Beatty: Let's talk a little bit about some of the topics for today. Today we were going to talk about the ethics and privacy and data, all this stuff with, you know, using and tank and education. And I asked you to read the there was an article on

181
00:22:47.830 --> 00:22:49.390
Brian Beatty: What was it on

182
00:22:49.960 --> 00:23:07.209
Brian Beatty: Jason and's work right? Some of you may have been familiar with that before, or things like that. Let me share that real quickly. Let's see. We're gonna take a break in about 10 min, just for you know, short, short, few minutes. Talk about this for a little bit, and the various other things, some kind of summary summarizing some ideas.

183
00:23:07.220 --> 00:23:14.110
Brian Beatty: What did you find from it? What did you think was interesting? Those kinds of things? And then what we're going to do is do a a a breakout room activity

184
00:23:14.380 --> 00:23:28.449
Brian Beatty: talking about how you evaluate LED Tech now, or have in your context or context, where the gaps are, maybe, and what could what might be done better, especially thinking about the various rubrics that you may have looked at.

185
00:23:28.460 --> 00:23:46.560
Brian Beatty: So what about this? You know you? This is not new. Obviously the tax on social media have been around for a while. We've had some kind of expose kind of things. There's been a few feature movies about it, and you know there are still ongoing, you know, hearings every now and then at the Congressional level and

186
00:23:46.570 --> 00:23:51.359
Brian Beatty: lawsuits it's in the news. Still, relatively frequently

187
00:23:52.090 --> 00:23:54.460
Brian Beatty: is this relevant for you and your work

188
00:24:02.840 --> 00:24:22.390
Brian Beatty: do well. Do you use social media in your in your training of efforts or your your educational efforts in the classroom like with teaching people things? Or does your does your your system use it like your school or your You know you with your parent, community.

189
00:24:22.400 --> 00:24:27.809
Brian Beatty: or your company for any, any inter inter interaction with with

190
00:24:29.200 --> 00:24:30.360
Brian Beatty: employees.

191
00:24:34.280 --> 00:24:41.710
Adam Hill (he/him/his): And do we define social media as like big social media? So things like the big like Instagram Facebook. It

192
00:24:41.780 --> 00:24:45.800
Brian Beatty: Yes, the bigger ones that are that are often

193
00:24:46.410 --> 00:24:53.170
Brian Beatty: drip free free to users. But gather using a lot of data and also basically

194
00:24:53.210 --> 00:24:59.320
Brian Beatty: manipulating basically well designed to attract your attention. Okay.

195
00:24:59.900 --> 00:25:09.159
Brian Beatty: right. Those are really the big companies. I mean, the small ones are probably moving, you know. They would like to do that, too. I mean, that's how they get bought by bigger companies. But

196
00:25:09.330 --> 00:25:17.769
Brian Beatty: yeah, I would say it's more, not the whole, not just the concept of people talking to each other. But the kind of the commercial side of this.

197
00:25:20.240 --> 00:25:26.230
Maristella Tapia: Let's see. Let's go by.

198
00:25:26.990 --> 00:25:34.630
Dalton Lobo Dias (he/him): Yeah, I mean my, my, my work at my school, we our internal Wi-fi firewall blocks.

199
00:25:34.860 --> 00:25:45.150
Dalton Lobo Dias (he/him): all the social media websites. Oh, the big ones right the the little ones, that kind of pop up here and there there's there's you know. It becomes kind of a game of.

200
00:25:45.320 --> 00:25:53.210
Dalton Lobo Dias (he/him): But we make it really clear for students that, like, you know, the computer that they have from school is meant for school work only.

201
00:25:53.470 --> 00:25:55.030
Dalton Lobo Dias (he/him): but you.

202
00:25:56.340 --> 00:26:07.229
Dalton Lobo Dias (he/him): you know they they get around with new websites, Pop it up all the time. We to our we can't like, just stop it. So there's only so much that we can do. So. Then we try to also coach our students that

203
00:26:08.660 --> 00:26:16.150
Dalton Lobo Dias (he/him): that You know these companies. They know how the brain works. They They've gotten really good at grabbing the attention they can. They're competing for their attention.

204
00:26:16.260 --> 00:26:19.820
Dalton Lobo Dias (he/him): But there's Of course there's only so much we can do there, too, because

205
00:26:20.070 --> 00:26:26.860
Dalton Lobo Dias (he/him): you know, a lot of students come back to me say like, yeah, it's great. What's the problem. So it's they. They don't have the perspective yet, or the

206
00:26:27.090 --> 00:26:35.190
Dalton Lobo Dias (he/him): self-control right? And every kid is a little bit different. So some are very aware of this that are going the other way, and then some kids are are

207
00:26:35.310 --> 00:26:38.449
Dalton Lobo Dias (he/him): when diving right in and being in that world.

208
00:26:38.470 --> 00:26:39.670
Dalton Lobo Dias (he/him): But

209
00:26:39.960 --> 00:26:43.959
Dalton Lobo Dias (he/him): you know I also see that a lot of the games that they are playing.

210
00:26:44.050 --> 00:26:48.020
Dalton Lobo Dias (he/him): adding more and more social elements. So

211
00:26:48.230 --> 00:26:51.920
Dalton Lobo Dias (he/him): kids want, you know, if they are playing a game they would love to be in that

212
00:26:52.060 --> 00:26:58.400
Dalton Lobo Dias (he/him): in the same world, right? So like roadblocks. Minecraft is really good at that. So it does create a sense of community.

213
00:26:58.560 --> 00:27:01.650
Dalton Lobo Dias (he/him): So I think there's something to be gained there. I don't think it's all bad.

214
00:27:02.890 --> 00:27:06.659
Dalton Lobo Dias (he/him): but it's it, you know this stuff is sort of happening.

215
00:27:07.440 --> 00:27:13.349
Dalton Lobo Dias (he/him): you know. I don't want to say in the shadows, but it's definitely outside of supervision of parents and schools like there's.

216
00:27:13.670 --> 00:27:17.870
Dalton Lobo Dias (he/him): you know there's not much we can do with that except really coaching students.

217
00:27:20.050 --> 00:27:32.509
Brian Beatty: Yeah, blocking it inside the the school is one thing around school based computers is one thing, but that doesn't mean that I mean it still can be used even in education, if students are using it with each other.

218
00:27:32.540 --> 00:27:48.389
Brian Beatty: or do an assignment that way, or the teacher uses it with students outside of class. Sometimes that happens, or they have assignments where they have to use these tools, even if they're not using them in school. So it's it. It can be kind of connected from lots of different places. Mari.

219
00:27:51.010 --> 00:27:56.480
Maristella Tapia: Yeah, I was just gonna say that you're asking if it's relevant in our work, and

220
00:27:56.880 --> 00:28:00.190
Maristella Tapia: it it is definitely something that

221
00:28:00.560 --> 00:28:10.609
Maristella Tapia: some of our some instructors use to kind of connect to sort of build community and classes like I know a colleague of mine uses like Instagram.

222
00:28:10.660 --> 00:28:29.919
Maristella Tapia: even in some, for some assignments. She has, like students, you know, post a story cause she teaches literature like post a a a haiku in your story for today, or something like that, and she kind of requires her students to to do to utilize social media in her assignments, and as a means to build community.

223
00:28:29.930 --> 00:28:37.579
Maristella Tapia: And you know I I've asked her about, you know, any push back, she's gotten, and she she does allow students to opt out, but

224
00:28:37.640 --> 00:28:41.169
Maristella Tapia: I think that it's really important for teachers to kind of

225
00:28:41.680 --> 00:28:43.979
Maristella Tapia: be aware of the fact that you know

226
00:28:44.430 --> 00:28:45.960
Maristella Tapia: you're asking

227
00:28:46.010 --> 00:28:55.619
Maristella Tapia: people to kind of potentially put themselves at risk for monitoring and data collection and and data, theft and mining, and all of that stuff, and

228
00:28:55.670 --> 00:29:03.690
Maristella Tapia: and making that be a requirement. Maybe, you know, we might want to reconsider that.

229
00:29:03.920 --> 00:29:13.569
Maristella Tapia: But this definitely I I found this article really really super interesting and also scary, but

230
00:29:13.710 --> 00:29:29.769
Maristella Tapia: it really definitely reminded me of, like the behaviorism talks that we had last semester. And just you know how psychology is used most recently, you know Tik Tok has been in the news because of the Chinese spy balloon and

231
00:29:29.780 --> 00:29:44.520
Maristella Tapia: tik Tok being an app that essentially has the capabilities to spy on. You know your your behavior, and you know your preferences and what you're doing and stuff like that. So I think that this is like

232
00:29:44.660 --> 00:29:47.460
Maristella Tapia: just really an important topic for everybody

233
00:29:47.500 --> 00:29:51.520
Maristella Tapia: to be aware of and think about and and how

234
00:29:51.780 --> 00:29:54.410
Maristella Tapia: we're impacted by.

235
00:29:55.070 --> 00:30:00.120
Maristella Tapia: You know, by these social media data companies really, that

236
00:30:00.170 --> 00:30:01.070
Maristella Tapia: that

237
00:30:01.360 --> 00:30:05.480
Maristella Tapia: are still emerging, like you know, like the article, says that

238
00:30:05.750 --> 00:30:06.849
Maristella Tapia: there are

239
00:30:06.900 --> 00:30:12.160
Maristella Tapia: things at the beginning. People were like not really knowing if there were

240
00:30:12.240 --> 00:30:17.280
Maristella Tapia: harmful side effects of these technologies. But now

241
00:30:17.290 --> 00:30:34.250
Maristella Tapia: for sure, they know that there are, you know, right that that that more and more has come out with regard to like the use of data and manipulation, psychological manipulation, all of that that. And you know self esteem impacts, and all all of those things that that this has on on.

242
00:30:34.310 --> 00:30:35.969
Maristella Tapia: on youth. So.

243
00:30:36.000 --> 00:30:46.759
Maristella Tapia: along with with digital literacy, I think there needs to definitely be a big part of digital literacy that is like social media literacy and like data literacy.

244
00:30:46.860 --> 00:30:49.130
Maristella Tapia: privacy, you know. Conversation

245
00:30:49.270 --> 00:31:03.569
Brian Beatty: Yeah, definitely well. And and the the power of tools like tik to to motivate behavior like in in the news lately around here. There's been the the stories about the you know, the the kids who are shooting people with the gel.

246
00:31:03.580 --> 00:31:09.319
Brian Beatty: the gel cap. It's a tik tok thing, right? So they take a video and they post it. I don't think I think it's a tech talk thing

247
00:31:09.420 --> 00:31:12.360
Brian Beatty: you know that like, or the frozen gel gel.

248
00:31:12.460 --> 00:31:19.679
Brian Beatty: you know, which is a which is a a thing that's been going on for a while. Now it's just kind of hitting the news. So you know it's been going on for a while.

249
00:31:20.900 --> 00:31:21.650
Brian Beatty: Adam.

250
00:31:23.070 --> 00:31:30.059
Adam Hill (he/him/his): Yes, similar to Mari, I, you know, working in higher LED, and with faculty they are designing their courses. A lot of them have assignments

251
00:31:30.170 --> 00:31:31.490
Adam Hill (he/him/his): for which

252
00:31:31.520 --> 00:31:34.399
Adam Hill (he/him/his): they're asking students to do something.

253
00:31:34.640 --> 00:31:47.739
Adam Hill (he/him/his): we on social media, and you know these are all for masters programs. So like like we Recently I worked on a course that was a public health promotion for nurses. So nurses like having nurses, do help promotion

254
00:31:48.040 --> 00:31:49.530
Adam Hill (he/him/his): activities

255
00:31:49.620 --> 00:31:54.889
Adam Hill (he/him/his): by using a social media post. But you see that a lot, because that is also like.

256
00:31:55.170 --> 00:32:04.190
Adam Hill (he/him/his): you know, the the professions that people are going into like, they need to learn how to use it. And so it kind of becomes this: Catch 22 in terms of ethics. Of like

257
00:32:04.410 --> 00:32:11.420
Adam Hill (he/him/his): You need to train future professionals to use social media, because that is going to be the medium for which they need to

258
00:32:11.910 --> 00:32:18.300
Adam Hill (he/him/his): sometimes do their jobs, but also, like the ethical risks that come with that.

259
00:32:18.320 --> 00:32:21.990
Brian Beatty: Yeah, if if you have to give students the option of opting out

260
00:32:22.270 --> 00:32:25.569
Brian Beatty: that auto that ought to raise at least a yellow flag for you

261
00:32:25.630 --> 00:32:36.780
Brian Beatty: that maybe we should. Maybe I really shouldn't be doing this, and maybe if you, if the idea is to get people to be more savvy around using social media or the tools. So for like a blog post.

262
00:32:37.080 --> 00:32:49.910
Brian Beatty: I mean I I When it was first a new, a new thing. I thought I had everybody, you know. Go create a blog, you know. Get a free account somewhere, create a block, share it with all of us, and boom, boom. You could delete it after the semester, and I only did that for one semester.

263
00:32:50.060 --> 00:32:58.769
Brian Beatty: because it was forcing people to use technologies that were with that were outside of the the kind of the protected zone of the university.

264
00:32:59.190 --> 00:33:15.239
Brian Beatty: So to learn how to write a blog, you could use a blog site that's within the protection of the university. Somehow. You know more of a private kind of space. It doesn't have the same kind of public exposure, you know. But if you had, if you had a course like in journalism, or something like that, where that was part of what you had to learn how to do.

265
00:33:15.470 --> 00:33:26.130
Brian Beatty: You? You might manage that differently, you know. Maybe students all have pseudonyms or something like that, and so they don't have to use their real, the real names. They're really emails, or any kind of like anything like that I don't know.

266
00:33:26.610 --> 00:33:28.589
Brian Beatty: But it does raise some questions. I think

267
00:33:31.100 --> 00:33:32.620
Brian Beatty: any other comments on this.

268
00:33:37.810 --> 00:33:42.409
Brian Beatty: All right. Well, let's take a 5 min break when come back. I'm going to ask you about

269
00:33:42.980 --> 00:33:45.580
Brian Beatty: the the Google Terms of service that

270
00:33:46.110 --> 00:33:48.829
Brian Beatty: we'll probably all agree to at some point, and

271
00:33:48.900 --> 00:33:56.800
Brian Beatty: probably nobody has ever read unless they possibly read it before before class and preparation for a discussion. Maybe you have. But let's take 5 min.

272
00:33:56.990 --> 00:34:02.309
Brian Beatty: I'm gonna pause the recording. We'll come back in 5 min and we'll. We'll. We'll talk a little bit more. Then we're gonna go into a breakout.

273
00:34:16.690 --> 00:34:18.549
Brian Beatty: Okay, we're coming back.

274
00:34:21.530 --> 00:34:22.489
Brian Beatty: Anybody out there.

275
00:34:24.350 --> 00:34:25.829
I don't want that.

276
00:34:28.590 --> 00:34:30.730
Brian Beatty: all right. We just had a little break. There.

277
00:34:36.850 --> 00:34:39.509
Brian Beatty: let's talk a little bit about

278
00:34:39.870 --> 00:34:41.679
Brian Beatty: the Google terms of service.

279
00:34:42.380 --> 00:34:52.099
Brian Beatty: And there are the other readings that you can. You know we can talk about some of that if you're interested in talking about it specifically, but I have a question for you.

280
00:34:52.820 --> 00:34:57.629
Brian Beatty: and the question is whether or not you've ever read the Google terms of service.

281
00:34:57.650 --> 00:34:59.020
Brian Beatty: I'm. Assuming that

282
00:34:59.940 --> 00:35:01.820
Brian Beatty: you probably all use this

283
00:35:01.900 --> 00:35:05.879
Brian Beatty: some level, and you may have a school that uses it with students.

284
00:35:09.850 --> 00:35:22.770
Brian Beatty: Right? Any of you. I have Google schools out there, you know, I read, or K. 12. You know, there's a lot of this use out there, and once you once you start using Google, Of course it's really hard not to use

285
00:35:23.580 --> 00:35:24.959
Brian Beatty: a lot of Google.

286
00:35:26.590 --> 00:35:27.560
Brian Beatty: Yeah.

287
00:35:28.700 --> 00:35:37.289
Brian Beatty: Yeah, we've all we all, we all we agree to these stuff, this stuff all the time. Oh, yeah, that's oh, yeah, let's just connect this. You would like us to connect you through your Google account.

288
00:35:37.320 --> 00:35:54.100
Brian Beatty: sure. Why not? It's so convenient. I don't have to log in anymore, or very little thing. It's so convenient, and they really make it that way which tells you there's there's a lot of the data sharing going on, and it's all well, it's sort of it some of times. It's talked about in here. If you go through and look at this.

289
00:35:54.140 --> 00:35:57.019
Brian Beatty: There's a lot of different areas to to kind of explore.

290
00:35:57.920 --> 00:36:01.179
Brian Beatty: Right? What? What's your relationship with Google.

291
00:36:01.280 --> 00:36:08.710
Brian Beatty: you know, using services, all the software all these other things the privacy policy, a whole, another area.

292
00:36:08.810 --> 00:36:15.549
Brian Beatty: the technologies. How do we? How does our technology work? How do we use cookies? How do we use location.

293
00:36:15.850 --> 00:36:20.830
Brian Beatty: How do we use your your credit card numbers, etc., etc.?

294
00:36:21.590 --> 00:36:22.439
Brian Beatty: So

295
00:36:23.910 --> 00:36:31.870
Brian Beatty: if you've read this, what is it? Did you find anything very surprising in this anything that surprised you. Didn't you didn't realize this was in there

296
00:36:33.470 --> 00:36:35.479
Brian Beatty: anything you were uncomfortable with.

297
00:36:43.430 --> 00:36:47.310
Dalton Lobo Dias (he/him): Oh, well, one thing that our our tech director

298
00:36:47.700 --> 00:36:49.680
Dalton Lobo Dias (he/him): tells our students every year

299
00:36:50.030 --> 00:36:56.339
Dalton Lobo Dias (he/him): just once, though, is that everything you put in a Google Doc, or a slide, or a spreadsheet, or whatever

300
00:36:56.640 --> 00:37:01.199
Dalton Lobo Dias (he/him): Google's. So like. If you're writing, let's say a novel, and you prepare to sell it.

301
00:37:01.570 --> 00:37:06.090
Dalton Lobo Dias (he/him): It's not private like they they have access to it. They can do whatever

302
00:37:06.380 --> 00:37:08.020
Dalton Lobo Dias (he/him): I don't know if that's really true. But

303
00:37:08.770 --> 00:37:10.829
Dalton Lobo Dias (he/him): you know I I trust I trust him on that.

304
00:37:11.000 --> 00:37:12.569
Brian Beatty: You trust him on that.

305
00:37:13.330 --> 00:37:15.960
Brian Beatty: Where would you look for that in terms of service.

306
00:37:18.740 --> 00:37:20.149
Dalton Lobo Dias (he/him): I guess.

307
00:37:23.130 --> 00:37:27.339
Dalton Lobo Dias (he/him): Like who has ownership, I guess, of the maybe in the FAQ.

308
00:37:27.580 --> 00:37:28.259
Yeah.

309
00:37:28.300 --> 00:37:30.959
Brian Beatty: is there a way to search this? I Haven't looked.

310
00:37:37.300 --> 00:37:39.509
Brian Beatty: There's not a lot of that. FAQ. Is there

311
00:37:43.480 --> 00:37:50.010
Brian Beatty: using services content and Google services your content, I think we may have found it

312
00:37:57.620 --> 00:38:00.229
Brian Beatty: intellectual property rights here.

313
00:38:01.180 --> 00:38:02.810
Brian Beatty: Google contents

314
00:38:04.170 --> 00:38:06.670
Brian Beatty: their contents other content

315
00:38:10.890 --> 00:38:16.509
Dalton Lobo Dias (he/him): that doesn't really talk about that. So on another page where it says, as acribed

316
00:38:17.980 --> 00:38:18.649
most.

317
00:38:18.790 --> 00:38:20.169
Dalton Lobo Dias (he/him): i'll put this in your check

318
00:38:23.230 --> 00:38:25.819
Dalton Lobo Dias (he/him): as described in the Google terms of service.

319
00:38:26.300 --> 00:38:33.080
Dalton Lobo Dias (he/him): your content remains yours. We do not claim any ownership of any of your content, including any text data information.

320
00:38:34.700 --> 00:38:37.220
Brian Beatty: That is, that is what I would hope to see there.

321
00:38:39.090 --> 00:38:49.749
Brian Beatty: I don't. I don't think it. I don't think it means they can't use your content to kind of improve their services, and to maybe even train their AI tools to sell those kinds of things.

322
00:38:49.870 --> 00:38:59.580
Brian Beatty: But at least they're telling you that they're not claiming your content as theirs. Is that is that that? So that is, is that not what your technology person was saying.

323
00:39:00.730 --> 00:39:01.810
Dalton Lobo Dias (he/him): Yeah.

324
00:39:01.860 --> 00:39:02.750
Dalton Lobo Dias (he/him): yeah.

325
00:39:02.960 --> 00:39:05.359
Dalton Lobo Dias (he/him): he was saying that, like whatever you put in there.

326
00:39:05.950 --> 00:39:07.989
Dalton Lobo Dias (he/him): you can assume that

327
00:39:08.090 --> 00:39:20.199
Dalton Lobo Dias (he/him): other people can can get access to it right because of the link sharing. Of course, that makes sense. But he was going as far as saying that, like whatever we put in there, Google owns, they can claim it if they wanted to.

328
00:39:20.590 --> 00:39:21.339
Dalton Lobo Dias (he/him): So

329
00:39:21.930 --> 00:39:33.569
Brian Beatty: doesn't seem like that's what i'm finding here. Yeah, I do. Yeah. And I would say that I don't think that's true. The the second part of that the scariest part of that where you're talking. Well, they own it now is not really true, but that doesn't mean they can't use it

330
00:39:34.490 --> 00:39:44.640
Brian Beatty: right. And and I think there may be more, some more specific language, especially maybe for those specific applications, perhaps, where it talks about specifically how they do have permission to use your data.

331
00:39:47.840 --> 00:39:52.670
Brian Beatty: Linda says. Why are we even? Why do we even care about this? Right? It's just Google.

332
00:39:52.960 --> 00:39:59.530
Brian Beatty: Google's everywhere. You can't get it if you're going to use a computer these days or a smartphone. You're going to be using Google. So you may as well just agree to it. Right?

333
00:39:59.540 --> 00:40:18.120
Dalton Lobo Dias (he/him): Yeah, I mean. Here's what I just became normal, just kind of sort of accepted. I totally. I know what Linda saying. I think my students are growing up like just in the Google ecosystem. Right? It's just a it's just totally normal like this.

334
00:40:18.130 --> 00:40:23.860
Dalton Lobo Dias (he/him): We don't talk about any danger or anything like that. But here's Here's 1 one way that this could play out

335
00:40:23.890 --> 00:40:30.490
Dalton Lobo Dias (he/him): that I remember a time like a few years ago, when teachers were really careful to not add last names to spreadsheets.

336
00:40:30.750 --> 00:40:33.400
Dalton Lobo Dias (he/him): but like now I notice that teachers do it all the time.

337
00:40:33.430 --> 00:40:36.089
Dalton Lobo Dias (he/him): especially when we're talking about like admission stuff

338
00:40:36.210 --> 00:40:37.350
Dalton Lobo Dias (he/him): or

339
00:40:37.430 --> 00:40:40.560
Dalton Lobo Dias (he/him): behavioral issues, or even health things

340
00:40:41.020 --> 00:40:58.699
Dalton Lobo Dias (he/him): like it's just it's just in spreadsheets. And these these spreadsheets that are in like Google sheets. Yeah, yeah, yeah. Yeah. And so I i'm wondering, like. You know what happened about that, because, you know, now there is a data, you know. Now, like, let's say, some really serious stuff about a student that probably

341
00:40:59.000 --> 00:41:00.420
Dalton Lobo Dias (he/him): good influence.

342
00:41:00.700 --> 00:41:17.529
Brian Beatty: future decisions, or hiring, or whatever it's it's there, right? Maybe comments on a disciplinary action. Or or, yeah, you know, a advising session or something like that that has sensitive information in all kinds of things. Yeah. Harassment, right? Right?

343
00:41:17.790 --> 00:41:22.560
Brian Beatty: Well, it's one reason why there are some other like the we use a. We use an advising

344
00:41:22.590 --> 00:41:30.839
Brian Beatty: platform here. That is, that is it's it's it's kind of within its own kind of fired world system. So it's not connected that way.

345
00:41:32.470 --> 00:41:34.050
Brian Beatty: but it. But if it

346
00:41:34.440 --> 00:41:49.519
Brian Beatty: if you built something like in the Google, the Google World, right because you could build some really cool systems that way to capture all this data, and you could put passwords on all of that. But still that doesn't mean it's not a stated that's not possibly exposed in some other way. Linda.

347
00:41:52.140 --> 00:41:54.089
Linda Kim: Oh, yeah. So

348
00:41:54.290 --> 00:41:55.179
Linda Kim: I work

349
00:41:55.470 --> 00:42:08.149
Linda Kim: again. I work for a company that's you know, that, like certain design or certain information, it's very sensitive. I work for some. I conduct a company that designs our own products so like that that

350
00:42:08.160 --> 00:42:19.100
Linda Kim: security it, as it security, is very important things. So we have a very strong firewalls, or any restriction on any sort of messengers, or even Google's walked.

351
00:42:19.210 --> 00:42:22.689
Linda Kim: and we can use the search engine by like any Google products. So

352
00:42:22.730 --> 00:42:24.669
Linda Kim: But

353
00:42:25.010 --> 00:42:37.140
Linda Kim: when I first started, this is my, for this is the first company that blocked Google Products or any data sharing or information sharing app. So, and I found myself being very.

354
00:42:37.190 --> 00:42:44.920
Linda Kim: I don't know it was very kind of hard to work around it, because I was like Google, for example, since we're talking about products.

355
00:42:45.140 --> 00:42:56.599
Linda Kim: it was so normal in my life using Google and for anything, not just email for everything. And suddenly this company tells me. Oh, you can't use this any more.

356
00:42:56.620 --> 00:42:58.089
Linda Kim: It was kind of very

357
00:42:58.200 --> 00:43:15.979
Linda Kim: kind of hard to readjust like. Learn how to like. Do my quoteable do my work without Google Products. So, message somebody like, I couldn't do Google chat anymore. I had to email, or I had to use our internal, you know, whatever, so I don't know. So

358
00:43:15.990 --> 00:43:20.140
Brian Beatty: to my point, like it's like quote unquote, so normal for us. This is like

359
00:43:20.190 --> 00:43:27.479
Brian Beatty: Well, it's true for the longest time. I always I always resisted that request to go. Do you want to just sign in with your Google account?

360
00:43:27.710 --> 00:43:36.090
Brian Beatty: And I just said, No, no, no, I'd rather have my own login and password and things like that, and I finally started to give it in a couple of months ago, and then I noticed

361
00:43:36.100 --> 00:43:53.929
Brian Beatty: that you know. Let's say i'm salt lining and signing into an account i'm doing i'm, doing, shopping, or whatever it has to be, you know not surprisingly. Immediately, and all these other applications I may be using. Where advertising is part of the stream. I start seeing the ads for the things I was looking for or related products or things I was looking for 6 months ago.

362
00:43:54.070 --> 00:43:58.809
Brian Beatty: So it's clear that there's so much data

363
00:43:58.840 --> 00:44:04.640
Brian Beatty: data sharing going on that it's almost like. Okay, I guess this is just the way the world is.

364
00:44:04.850 --> 00:44:17.070
Brian Beatty: I don't know if there's a way to not not put yourself in that situation. If you're going to use these kind of technologies to to to manage your life, and they make it so much more convenient in some ways to do the things we want to do.

365
00:44:17.660 --> 00:44:31.129
Brian Beatty: So yeah, for example, we do. You know, when we do, we have. We have kids, and when they have birthday they're they're not. They don't live with us anymore. And so we say, hey, you know, put a list up somewhere, and so we use this app called. I think it's called Elster.

366
00:44:31.140 --> 00:44:39.629
Brian Beatty: It was designed around Christmas giving. But we you do for birthdays. You know all that kind of stuff, and so they put things on there, and of course they can link things right into Amazon.

367
00:44:39.830 --> 00:44:46.960
Brian Beatty: Right? Well, once you start linking into these, these, you know, just click here to purchase it. You know there's a lot of data that's being

368
00:44:47.070 --> 00:45:02.289
Brian Beatty: being collected in past, even if it's it's you know, innocuous. But still it's data that's being shared about you that other people are actually generating revenue from, and that you never see. Of course, your the revenue you get the payback you get, I guess, is the use of the tool. So for convenience.

369
00:45:04.580 --> 00:45:07.590
Stephen Cadette: anything else on how commercials are

370
00:45:08.140 --> 00:45:09.140
Brian Beatty: commercials.

371
00:45:09.970 --> 00:45:12.570
Stephen Cadette: I know. I yeah.

372
00:45:13.420 --> 00:45:15.149
Stephen Cadette: You know you're You're the product

373
00:45:15.810 --> 00:45:17.229
Brian Beatty: for a commercial.

374
00:45:17.820 --> 00:45:21.409
Stephen Cadette: I have billboards ads on.

375
00:45:21.450 --> 00:45:23.019
Stephen Cadette: you know, bus stand.

376
00:45:23.940 --> 00:45:34.329
Stephen Cadette: You're the You're the target. You're the product, and you know you're passing by these things, and you're not making any money from it. But I think that's sort of what selling your data is.

377
00:45:34.740 --> 00:45:35.619
Stephen Cadette: in a way.

378
00:45:35.960 --> 00:45:37.609
Stephen Cadette: anyway. Different conversation.

379
00:45:37.770 --> 00:45:44.540
Brian Beatty: Well, no. But I think you're right? Because what? What is the billboard company? What do they? What? What is the value they provide to the people who pay them

380
00:45:44.780 --> 00:45:49.579
Brian Beatty: your cost, your attend. They they pay. They're They're providing your attention

381
00:45:49.650 --> 00:45:53.360
Brian Beatty: right to the to whatever that message is. As you're driving by.

382
00:45:54.300 --> 00:45:58.179
Brian Beatty: or as you're sitting in a car. Someone else is going by. Yeah, anyway.

383
00:45:58.410 --> 00:46:00.880
Brian Beatty: Yeah, it's it's it's attention.

384
00:46:01.360 --> 00:46:05.079
Brian Beatty: Essentially, it's we you've heard. Probably the term the attention economy.

385
00:46:05.370 --> 00:46:22.380
Brian Beatty: That's exactly what it is right. They're basically paying for your attention. Why, so they can. They can motivate your behavior, and it's usually your behavior to to buy something on a commercial side. But it's not always just buying something. Maybe it's making decisions differently convincing you of a position, a political position or a social position.

386
00:46:22.500 --> 00:46:23.969
Brian Beatty: All that stuff

387
00:46:24.440 --> 00:46:27.160
Brian Beatty: that's what they're paying for, though it's your your attention.

388
00:46:27.220 --> 00:46:30.709
Brian Beatty: That's why you know you sign up for

389
00:46:30.820 --> 00:46:48.739
Brian Beatty: a rewards program at some place you can be sure that that email address you're using is being sold in some list somewhere to someone, for some reason that you're gonna get emails asking you to buy things. It's not just Usually it's not just from that Rewards club, for I don't know Goldstone, ice cream or something like that.

390
00:46:51.070 --> 00:46:55.160
Brian Beatty: Yeah, Adam. And the phones. I think there's a lot that's possible.

391
00:46:55.460 --> 00:46:58.880
Brian Beatty: technically, that most of us have no idea.

392
00:46:58.990 --> 00:47:05.090
Brian Beatty: and unfortunately, sometimes we have to wait until it gets exposed in some way. Then all of a sudden. Oh, my gosh!

393
00:47:05.360 --> 00:47:10.090
Brian Beatty: This is this: we find this has actually been going on for a long, long time

394
00:47:10.750 --> 00:47:12.189
Brian Beatty: with no controls around it.

395
00:47:14.820 --> 00:47:18.709
Maristella Tapia: Okay. I also put in the chat that

396
00:47:18.920 --> 00:47:33.449
Maristella Tapia: and Linda kind of confirmed this: that there's definitely been times that I've been having conversation with my husband, or like a friend of mine, and i'll mention something. And then the next moment i'll open like

397
00:47:33.580 --> 00:47:36.270
Maristella Tapia: my Google like

398
00:47:36.280 --> 00:47:58.079
Maristella Tapia: Browser or my Instagram, and it'll be an AD for something that I was talking about, or like. I'll have a conversation with my husband about going to like on vacation somewhere, and then it'll be like oh, tickets to you know San Diego, or like, or whatever it is that I just mentioned, and I wouldn't have done any searches. I wouldn't have entered this information any other way.

399
00:47:58.240 --> 00:48:02.880
Maristella Tapia: It would literally just be a a a verbal conversation.

400
00:48:03.000 --> 00:48:12.260
Maristella Tapia: verbal, not through technology or with that technology just like in like in the room talking about certain things.

401
00:48:12.330 --> 00:48:14.130
Maristella Tapia: and then

402
00:48:14.280 --> 00:48:31.299
Maristella Tapia: and then you know it'll pop up on like his. Facebook will have something like some AD that was just about something he was talking about earlier or something, and it's not like you searched anything. So I I really wonder you know the extent to which some of these apps.

403
00:48:31.560 --> 00:48:33.629
Maristella Tapia: It can actually

404
00:48:33.890 --> 00:48:47.969
Maristella Tapia: listen in on conversations. I don't i'm not trying to get too conspiratorial, but it's definitely happened, and it's made me wonder how what kind of spine capabilities a lot of these apps have.

405
00:48:48.120 --> 00:48:50.330
Yeah, yeah.

406
00:48:50.550 --> 00:48:58.469
Maristella Tapia: But i'm serious. I I I get the algorithm thing, but it's like it'll just be of an in-person conversation.

407
00:48:58.840 --> 00:49:00.330
Maristella Tapia: And

408
00:49:00.610 --> 00:49:08.179
Maristella Tapia: i'm trying to remember a specific example that was so creepy it was very eerie, and i'm sure other people had examples like this.

409
00:49:08.390 --> 00:49:17.079
Maristella Tapia: Yeah, where it's kinda like. Oh, my husband was talking about like this cooking tool that he wants to buy that has like

410
00:49:17.190 --> 00:49:19.529
Maristella Tapia: it allows you to

411
00:49:19.580 --> 00:49:31.159
Maristella Tapia: kinda like finally, like grade, some kind of like ginger or garlic or other things that are kind of like really like really a pain in the butt to get really finely ground.

412
00:49:31.170 --> 00:49:44.889
Maristella Tapia: and you know, just sort of mentioning it. And then, you know, he opens up his Facebook, and there's an AD for it, or something like it's just. It was something really crazy like that where it's just it's too uncanny.

413
00:49:45.050 --> 00:49:51.620
Brian Beatty: It's too too strange. It has to be true. I don't who knows? Yeah, maybe. And maybe there's a pattern

414
00:49:51.680 --> 00:50:14.860
Brian Beatty: of I don't know words being used or Internet searches or things like that that lead them to believe that. Oh, this person wants it wants a a integrator, something like that. I have no idea. I do know that so more and more devices I have listening capability than ever before. We talk about the Internet of things right, and I find in I don't go out and look for you smart devices.

415
00:50:14.870 --> 00:50:29.119
Brian Beatty: But whenever I, when i'm upgrading things now almost always, there's a smart device option like I. I have a relatively new vehicle, and I can control some things on that vehicle from around the world as long as I'm. Online, and it can communicate to me

416
00:50:29.130 --> 00:50:41.930
Brian Beatty: it can. You know, You know cameras, you know external cameras. I can talk through those they can. They? They capture audio as well. I have a new thermostat in my house which senses when we're home.

417
00:50:42.010 --> 00:50:55.900
Brian Beatty: I don't think it does it through noise, but it it it easily could. You can have voice, control things any any time. Is voice controlled means. It's possible it could actually be listening, and that could be generating data for somebody to use in some way.

418
00:50:56.370 --> 00:51:05.460
Brian Beatty: Yeah, right? And so I have to have a whole new area of apps on my phone. Okay, these are like home home. These are the systems that I can control things at home.

419
00:51:05.760 --> 00:51:11.390
Brian Beatty: I didn't think i'd actually ever ever end up really kind of doing that. But you know you're buying new stuff now like

420
00:51:11.660 --> 00:51:13.889
Brian Beatty: refrigerators stoves all that stuff

421
00:51:14.010 --> 00:51:17.239
Brian Beatty: I do like the fact that I can start my car when i'm in a cold place

422
00:51:17.260 --> 00:51:19.040
Brian Beatty: and have it run for 10 min to warm up.

423
00:51:20.490 --> 00:51:22.250
Brian Beatty: But that's beside the point.

424
00:51:22.900 --> 00:51:36.419
Brian Beatty: So I think the point is that I think that if you're going to be using these these kinds of tools, these kind of actually any of these technologies. Someone should be checking this. Someone should be reading through these at least once.

425
00:51:36.450 --> 00:51:54.360
Brian Beatty: and whether it's you or it's a technology specialist, or if it's a it's a you know, a. You know, higher level like for us. We have a system, level technology group, you know they often will do contracts, and they're supposed to be checking through all this stuff, you know, legally, ethically, accessibility wise, all those kinds of things someone has to be doing that.

426
00:51:54.430 --> 00:52:13.290
Brian Beatty: and if it's not if it's not them, then it may not be getting done. And it might be interesting, especially if you got students. Who are, you know old enough, and have skills enough to understand some of the language in here to have them go through this at some point. Say, Well, this is what this is, what you're agreeing to when you're using this tool. Do you know what that means?

427
00:52:13.300 --> 00:52:19.310
Brian Beatty: Not to scare them, but maybe to make sure that they're hopefully making more informed decisions

428
00:52:19.480 --> 00:52:34.349
Brian Beatty: about what they're using, and how they're using it, what they're using it for. I always just told my kids look anything you type into a computer and send anywhere. You can just assume that it potentially could, could, you know? Go, go, go, you know, go public in some way someone could get access to it to just be aware.

429
00:52:34.390 --> 00:52:39.379
Brian Beatty: don't don't do stuff that you wouldn't want necessarily someone else to see. Someday that you didn't intend to see.

430
00:52:42.490 --> 00:52:49.600
Brian Beatty: Yeah, I think Google has been probably been forced to make this a lot more accessible for people in in in regular language.

431
00:52:49.720 --> 00:52:56.649
Brian Beatty: and if not forced by the course forced by the threat of the the potential threat, perhaps of legal action.

432
00:52:56.960 --> 00:52:59.370
And it's not like I don't think you know

433
00:52:59.560 --> 00:53:08.590
Brian Beatty: I I think I think they've done a lot for people using technology, but you know it's it's still a a business, and they're in the business of making money.

434
00:53:08.600 --> 00:53:20.769
Brian Beatty: and so that's part of what they do is they find ways to re to up, to get revenue, and if we're not paying specifically for it. Then we're paying for it in other ways. It's usually the way it goes. Maybe not always, but it seems to me

435
00:53:22.430 --> 00:53:23.509
Brian Beatty: all right.

436
00:53:23.540 --> 00:53:36.270
Brian Beatty: So just to this to stop. Let's stop this for a moment, and what i'd like to do on this kind of note is, ask you to get into some smaller groups and talk about what your organizations are doing

437
00:53:37.160 --> 00:53:52.699
Brian Beatty: to do this kind of review. How do you review technology that you're going to be using it Doesn't have to be social technology. It could be any kind of technology using to support learning or or really support operations, you know. But there's there's those rubric that have posted

438
00:53:52.790 --> 00:53:54.649
Brian Beatty: we able to access those

439
00:53:55.670 --> 00:54:00.269
Brian Beatty: I know. Sometimes those links were old, but when they came over from Ireland they still had the I learned tang in there.

440
00:54:00.390 --> 00:54:03.570
Brian Beatty: and I'm. I'm not sure. I hope that you were able to get to them

441
00:54:04.320 --> 00:54:06.539
Brian Beatty: if you tried to click them. Yeah, Good

442
00:54:06.650 --> 00:54:14.199
Brian Beatty: as I'm, i'm gonna change those links because the files are actually in the canvas system. Now, I just think it just remembers the old. I learned links.

443
00:54:14.230 --> 00:54:16.279
Brian Beatty: I believe that that's why it's there

444
00:54:16.380 --> 00:54:18.879
Brian Beatty: to clean it up yet.

445
00:54:19.100 --> 00:54:23.329
Brian Beatty: So let's let's do a breakout. So i'm gonna go to the breakout notes Here.

446
00:54:23.440 --> 00:54:27.339
Brian Beatty: let's see where is it probably have to change windows?

447
00:54:38.940 --> 00:54:42.379
Brian Beatty: There it is! What what am I? What am I sharing here?

448
00:54:44.190 --> 00:54:44.979
Nothing.

449
00:54:45.160 --> 00:54:47.519
Brian Beatty: Okay. I'm going to share the screen real quickly.

450
00:54:49.790 --> 00:55:08.259
Brian Beatty: So thank you, Ingred, for participating in the the notes from last week. I I added a few comments to there, for so those of you who are watching the recording and doing this asynchronously. Please add your own notes to our conversation now for session 2 as well, and take a look back if you want to see what I how I commented on it. But

451
00:55:08.560 --> 00:55:16.569
Brian Beatty: I just have 3 questions for you to kind of fill in the blank for you. You can fill in the blank with as many words as you want. But you know this is what i'd like you to talk about. You know in your setting.

452
00:55:16.640 --> 00:55:23.670
Brian Beatty: How do you review educational technology or instructional technology, or just technology that your your your people are going to be using

453
00:55:23.710 --> 00:55:29.240
Brian Beatty: because you probably using similar approaches. For you know, system level technology, you know.

454
00:55:29.910 --> 00:55:39.900
Brian Beatty: admin technology as well as learning technology. Maybe maybe there's a few extra things you might look for for the learning technology. What what should you be paying more attention to? What do you? Where do you think the gaps are.

455
00:55:39.950 --> 00:55:41.989
Brian Beatty: and what are some of the challenges that

456
00:55:42.270 --> 00:55:46.580
Brian Beatty: you have for implementing a more robust review

457
00:55:46.650 --> 00:55:47.729
Brian Beatty: process.

458
00:55:47.990 --> 00:56:03.069
Brian Beatty: evaluating to see whether or not a technology, how a technology will, you know, will be used, or whether it's the right technology for you. And what safeguards might you need around that? What are the risk? Factors? Sometimes you can't minimize or you can't get rid of all the risk for.

459
00:56:03.160 --> 00:56:09.580
Brian Beatty: or you know, data, leakage, or privacy, violations or things like that. Sometimes it's worth more than the risk. But how would you make that?

460
00:56:10.020 --> 00:56:11.660
Brian Beatty: How do you make that decision?

461
00:56:11.790 --> 00:56:31.220
Brian Beatty: Okay? So there's place for 4 groups, and I think we have enough people for 4 groups. So I think we'll have a groups of 3. There might be a group of 2 out there, but that's small enough that you get a chance to talk with we'll. We'll do this for 15 min no more, and we'll come back and debrief for about 10 or 15 min, and that should be that should take us about to the end of the evenings

462
00:56:31.280 --> 00:56:33.109
Brian Beatty: class. Any questions?

463
00:56:36.750 --> 00:56:50.889
Brian Beatty: No questions. Okay? Well, i'm going to pause the recording, and then we'll go into breakout rooms, and i'll start the recording, especially if you remind me to when we come back from our breakout session. The recording so I don't forget.

464
00:56:51.550 --> 00:56:54.299
Brian Beatty: So we're going back from our breakout.

465
00:56:54.820 --> 00:56:57.020
Brian Beatty: 2 groups are still out there chatting.

466
00:56:57.520 --> 00:57:01.469
Brian Beatty: Who was back first. You guys get to go first. Who was that? I don't remember

467
00:57:05.640 --> 00:57:08.200
Brian Beatty: It's like Room 2 is the first group. That's all back.

468
00:57:10.290 --> 00:57:12.969
Brian Beatty: One is partially back here. Here we are.

469
00:57:15.610 --> 00:57:16.580
Brian Beatty: Okay.

470
00:57:18.410 --> 00:57:20.920
Brian Beatty: everyone's back in the main room.

471
00:57:21.150 --> 00:57:26.419
Brian Beatty: We're recording again. Just Fyi. Let's hear from group 2.

472
00:57:26.450 --> 00:57:29.870
Brian Beatty: Make this a little bit easier for me to see your share.

473
00:57:35.000 --> 00:57:35.950
Brian Beatty: Group

474
00:57:36.150 --> 00:57:37.149
Brian Beatty: 2,

475
00:57:38.480 --> 00:57:41.110
Danny Cheng: all right. I will speak for the group 2.

476
00:57:41.450 --> 00:57:49.090
Danny Cheng: So for the first one we're talking about in our settings, we review at Tech currently like from my

477
00:57:49.650 --> 00:57:51.029
from me.

478
00:57:51.420 --> 00:57:57.069
Danny Cheng: I will say I work with the Ssa Registers office. So whenever we are looking at new policy, new

479
00:57:57.280 --> 00:58:02.059
Danny Cheng: technology or process, we're always thinking of first places that could this thing.

480
00:58:02.140 --> 00:58:09.389
Danny Cheng: you know, work with the current policy and protocols for the students data security. Could it work with data, with with the fret out.

481
00:58:09.890 --> 00:58:15.219
Danny Cheng: you know, because student. Piracy is a big thing to our office. That's for me.

482
00:58:15.520 --> 00:58:21.450
Danny Cheng: And Saturday was talking about that. You know she was working with students like, you know. Let's say, with a camera.

483
00:58:21.500 --> 00:58:26.890
Danny Cheng: or maybe you know, they enough. Students might not be feeling comfortable to being front of cameras.

484
00:58:27.360 --> 00:58:31.730
Danny Cheng: and Steve was also talking about that front keys

485
00:58:33.400 --> 00:58:35.149
Danny Cheng: experience. The

486
00:58:36.140 --> 00:58:49.550
Danny Cheng: he is kind of being the person that you know, can actually, in order to keep the cost and lessons goings like you know he he sometimes kind of moving forward, just, you know, allowing students to work with

487
00:58:50.040 --> 00:58:58.169
Danny Cheng: new tech new tech new technologies and not really necessarily focusing on data security. But then focusing on like you know.

488
00:58:58.490 --> 00:59:03.260
Danny Cheng: you, in using this technology to help students have better learning experience.

489
00:59:05.530 --> 00:59:11.260
Danny Cheng: That's from us, from the first one and then the second one we should pay more attention.

490
00:59:11.560 --> 00:59:15.259
Danny Cheng: And I will say for me is that

491
00:59:15.330 --> 00:59:19.649
Danny Cheng: we definitely should pay more attention on testing and also thinking about

492
00:59:19.910 --> 00:59:28.019
Danny Cheng: what could be some possible negative consequences with the technology like whatever. There's a loop host for users.

493
00:59:28.120 --> 00:59:33.279
Danny Cheng: and you know what? If somebody did something this way. Could it be a new problem?

494
00:59:33.890 --> 00:59:38.330
Danny Cheng: And Saturday was talking about, You know, monitoring young

495
00:59:38.400 --> 00:59:42.309
Danny Cheng: learners like you know, younger kids, They're in the classes.

496
00:59:42.390 --> 00:59:46.900
Danny Cheng: and no so like sound with threats around, you know, online.

497
00:59:48.330 --> 01:00:00.479
Danny Cheng: And Steve's talking about like you know how that how the students data could be used, like, you know if the students, their information, are being collected.

498
01:00:00.660 --> 01:00:04.889
Danny Cheng: You know. How are somebody who, having this information, how they we, they, we use it

499
01:00:04.980 --> 01:00:05.720
Brian Beatty: bye.

500
01:00:05.840 --> 01:00:15.540
Danny Cheng: And finally, the last one our challenges to more robust. In in fact, technology valuations for me. I point out that

501
01:00:15.720 --> 01:00:24.349
Danny Cheng: it will be good to whenever there is a new tech, new tag, a new process taking place. So it's a good idea to have a time and space

502
01:00:24.550 --> 01:00:29.900
Danny Cheng: for like a testing and work, even creating a testing environment just before

503
01:00:29.990 --> 01:00:41.939
Danny Cheng: this technology or process take place in the real time, and also having, like, you know, a feature to collect users feedback using those feedback to make improvements.

504
01:00:42.150 --> 01:01:00.669
Brian Beatty: and sadly, a point of something similar, you know, that is, having like a assessment for the process is the process actually working? Do you have a process, and are you using it? And is it effective? Those are 3 good questions to to kind of consider asking. So, Danny, let me ask you, the does the register make its own decisions on adopting technology?

505
01:01:01.160 --> 01:01:08.060
Danny Cheng: Not all of them, unfortunately, but like. Sometimes we will be part of those

506
01:01:08.170 --> 01:01:14.079
Brian Beatty: like meetings, or providing some of the you know recommendations consulting. Yeah.

507
01:01:14.150 --> 01:01:18.520
Brian Beatty: And even when you want to bring in a new technology, if you have to pay for it in particular.

508
01:01:18.580 --> 01:01:21.540
Brian Beatty: There's a whole university process to go through Isn't there.

509
01:01:21.830 --> 01:01:22.979
Danny Cheng: Yeah.

510
01:01:23.010 --> 01:01:27.550
Brian Beatty: So it actually has to go through it. Security Review. It has to go through.

511
01:01:27.990 --> 01:01:39.240
Brian Beatty: It has to have some level of accessibility. Review from the to the Dprc. And places like that. So so the University has tried to put some safeguards in place. So units

512
01:01:39.580 --> 01:01:46.309
Brian Beatty: don't go and kind of like, Go completely rogue, and just get whatever technology they want to use and have it violate all these.

513
01:01:46.320 --> 01:01:59.970
Brian Beatty: You know the guidelines, the rules sometimes the you know the loss, but your your organization is probably more more attuned to Ferpa and related, You know a sense around

514
01:01:59.980 --> 01:02:11.730
Brian Beatty: the date, the use of data, what data can be gathered? How can it be shared, etc.? Because most of most of the rough. So it's just have a very surface level, I think understanding of what purpose is, and actually it's it can get a little complicated.

515
01:02:11.860 --> 01:02:15.390
Brian Beatty: depend upon the technologies and the uses that are involved

516
01:02:16.250 --> 01:02:17.309
Brian Beatty: purpose they

517
01:02:17.870 --> 01:02:21.469
Brian Beatty: but something about student privacy rights.

518
01:02:21.620 --> 01:02:22.540
Brian Beatty: Yeah. So

519
01:02:22.960 --> 01:02:25.900
Brian Beatty: I don't even know what I I just was just reading it the other day, and I

520
01:02:26.250 --> 01:02:27.260
Brian Beatty: I forgot.

521
01:02:27.480 --> 01:02:32.950
Danny Cheng: Well, it's the Federal Government's law about the students privacy. Yeah, right? Right?

522
01:02:34.120 --> 01:02:34.939
Brian Beatty: Okay.

523
01:02:35.680 --> 01:02:37.280
Brian Beatty: Anything else from Group 2,

524
01:02:39.050 --> 01:02:40.309
Brian Beatty: you guys good with all that.

525
01:02:43.390 --> 01:02:56.429
Brian Beatty: Yeah, all that bad behavior online that stuff happens. It's a they don't make that stuff up. It happens, and you may not think it's going to happen in your situation with your students at your school, and you know until it does, and it's it's often very supervisor. Wow.

526
01:02:56.760 --> 01:02:58.040
Brian Beatty: Did not expect that.

527
01:02:58.270 --> 01:02:59.879
Brian Beatty: Let's go up to group one.

528
01:03:03.930 --> 01:03:07.749
Brian Beatty: Here's the names to remind you in the Joanna that right?

529
01:03:08.310 --> 01:03:10.120
Brian Beatty: Who's your? Who's your spokesperson?

530
01:03:11.870 --> 01:03:12.879
Everett Estkowski: I

531
01:03:13.590 --> 01:03:15.850
Everett Estkowski: I wouldn't write much for the first one.

532
01:03:16.150 --> 01:03:23.509
Brian Beatty: Joanna might want to talk about it a little bit. She's up for it. So this is why I ask every group pick someone to start talking.

533
01:03:24.030 --> 01:03:27.159
Brian Beatty: and almost no one ever does. That's okay.

534
01:03:27.710 --> 01:03:28.779
Brian Beatty: Group one.

535
01:03:31.300 --> 01:03:32.490
Everett Estkowski: So

536
01:03:34.500 --> 01:03:39.370
Everett Estkowski: did you? What was it here? Excuse me, I can. I can start I can mention.

537
01:03:42.820 --> 01:03:49.430
Everett Estkowski: We start radio. I just let you know bad radio here.

538
01:03:50.660 --> 01:03:52.449
some functionality

539
01:03:52.500 --> 01:03:54.620
Everett Estkowski: tools that are easy to use.

540
01:03:55.040 --> 01:03:56.479
Everett Estkowski: user friendly

541
01:03:56.680 --> 01:04:06.640
Everett Estkowski: for students and instructors to use and accessibility accessible to the students, free of charge or requiring extra fees.

542
01:04:07.710 --> 01:04:08.810
Everett Estkowski: So

543
01:04:09.340 --> 01:04:14.740
Everett Estkowski: for the second one we started getting into the security and

544
01:04:15.490 --> 01:04:18.580
Everett Estkowski: ethics of data collection.

545
01:04:18.780 --> 01:04:25.509
Everett Estkowski: I was thinking what it's used for. You know we were just talking about potentially, you being used to manipulate people without their knowledge of it

546
01:04:25.670 --> 01:04:33.920
Everett Estkowski: that right there something to address and talk about, and maybe even come up with laws for the these things. So

547
01:04:34.010 --> 01:04:44.269
Everett Estkowski: I I thought that that's paying more attention to it and the challenges, you know addressing it? You know it's it's cost labor resources.

548
01:04:44.880 --> 01:04:46.229
Everett Estkowski: and

549
01:04:47.380 --> 01:04:54.950
Everett Estkowski: you know, bringing it to the public's attention. How do you do that, you know, like you said sometimes by the time it gets to the news it's already been

550
01:04:55.010 --> 01:04:56.820
Brian Beatty: around for a while. But

551
01:04:57.190 --> 01:04:58.069
Everett Estkowski: so

552
01:05:01.010 --> 01:05:14.069
Brian Beatty: imagine how difficult this is for someone who's got oversight over this, I mean, think of we. We expect our, you know, our Federal Government or State governments to protect just in ways. But imagine how difficult that is for them to actually. Do

553
01:05:14.420 --> 01:05:28.369
Brian Beatty: you know, the stuff we hear about is big picture stuff, and i'm not saying it's not important. But there's so much other stuff that happens that they're not even they're probably not even aware of why, because it hasn't come up in their hearings because they haven't thought to ask for, because they don't really know what the tech people are doing.

554
01:05:30.730 --> 01:05:32.569
Brian Beatty: Maybe that's a negative perspective on it.

555
01:05:33.080 --> 01:05:35.719
Brian Beatty: Anything else? How about how about Section 3 here

556
01:05:37.300 --> 01:05:38.640
Brian Beatty: challenges?

557
01:05:40.500 --> 01:05:54.619
Linda Kim: I can. I can. Okay. So tying back to your comment after video like, who's gonna do all that right? So that's so. That's where we, you know, Start talking about it. The costs or general resources in general like. Is that realistic?

558
01:05:54.680 --> 01:06:07.359
Linda Kim: To be honest. I mean to the certain point, maybe, but you know, to have like a like at the perfect world kind of thing. It may be a little bit hard to bring it to reality, because all that right?

559
01:06:07.460 --> 01:06:08.089
Right?

560
01:06:08.370 --> 01:06:10.430
Linda Kim: So that's

561
01:06:11.060 --> 01:06:14.640
Linda Kim: the perfect for all people like to

562
01:06:14.840 --> 01:06:15.580
Linda Kim: right.

563
01:06:17.030 --> 01:06:20.969
Brian Beatty: Okay, thank you. Group one. Let's move on to group 4.

564
01:06:21.400 --> 01:06:24.129
Maristella Tapia: We'll do the group 3 last.

565
01:06:24.820 --> 01:06:34.569
Maristella Tapia: So it's just Amanda and myself in our group, and it took a while for us to answer this question, because the reality is is that

566
01:06:34.940 --> 01:06:43.030
Maristella Tapia: these decisions are pretty far removed from most from teachers and faculty at both of our institutions. And so

567
01:06:44.930 --> 01:07:00.380
Maristella Tapia: I did I. The more I thought about it. I do realize that we're part of a consortium of community colleges that make decisions about what kinds of, for example, to switch to canvas, or, you know.

568
01:07:00.580 --> 01:07:08.340
Maristella Tapia: web conferencing like zoom and other app integrations that are used in canvas things like

569
01:07:11.340 --> 01:07:13.770
Maristella Tapia: turn it in and stuff like that.

570
01:07:13.820 --> 01:07:26.970
Maristella Tapia: And so those a lot of those decisions are made at the Chancellor level of like, you know, there's like a Chancellor of instructional technology or a chance or online learning. And

571
01:07:27.040 --> 01:07:41.680
Maristella Tapia: so all of those decisions tend to be made at like that really higher level. And then the community colleges just tend to adopt what has been approved and vetted. But the most recent conversation I remember having that that was more kind of

572
01:07:41.770 --> 01:07:52.209
Maristella Tapia: in the realm of faculty, decision making and and debate was the use of a software called Proctorio, and

573
01:07:52.240 --> 01:07:56.549
Maristella Tapia: that software got a very like, you know our our

574
01:07:56.680 --> 01:08:01.219
Maristella Tapia: our college was up to renew if the license

575
01:08:01.580 --> 01:08:14.779
Maristella Tapia: and many faculty started to say that it, like factorial, is kind of racist, because it doesn't really do a good job of recognizing

576
01:08:15.020 --> 01:08:21.920
Maristella Tapia: faces that are non-white, and and so it can flag, for example, like

577
01:08:22.029 --> 01:08:24.210
Maristella Tapia: a person.

578
01:08:24.729 --> 01:08:26.649
Maristella Tapia: for you know

579
01:08:26.880 --> 01:08:35.809
Maristella Tapia: it like it's, it's so it's a it's a. It's a surveillance software where students record themselves taking an exam.

580
01:08:35.960 --> 01:08:44.919
Maristella Tapia: And so the the you know there are problems with it differentiating between different faces in terms of race, but also

581
01:08:45.100 --> 01:08:50.630
Maristella Tapia: it's especially difficult, because, like, if you know in multi person households.

582
01:08:51.200 --> 01:08:54.119
Maristella Tapia: you know, it can flag

583
01:08:54.180 --> 01:09:10.269
Maristella Tapia: a test If somebody walks into a room, you know, if you're taking it and and other things. So people just kind of were saying that it was very non equitable like it was just. You know it had the bill. That

584
01:09:10.279 --> 01:09:23.089
Maristella Tapia: effect of essentially discriminating against you know, lower income households, non white households. Even people who have certain disabilities who might, you know, move

585
01:09:23.439 --> 01:09:32.269
Maristella Tapia: slowly or differently, because it kind of monitors like movement. And then also just in general kind of violated student privacy.

586
01:09:32.840 --> 01:09:34.510
Maristella Tapia: So that was one thing.

587
01:09:34.689 --> 01:09:40.059
Maristella Tapia: Amanda also said, Can you scroll down that?

588
01:09:40.180 --> 01:09:53.929
Maristella Tapia: Yeah. Her institution, as far as she knows, like a lot of the tech decisions, are poorly kind of maintained, like students, lose some of their devices all the time, and can get new ones without really having to go through much of a process.

589
01:09:54.390 --> 01:10:12.109
Maristella Tapia: And then we talked about. We hope they would pay attention to privacy, protections, and intellectual property rights like, I think there was also a discussion around. Turn it in, and how turn it in actually does own papers that are turned in. There's some kind of clause about that or something that

590
01:10:12.450 --> 01:10:13.690
Maristella Tapia: did you hear that?

591
01:10:14.200 --> 01:10:20.809
Maristella Tapia: They? I I know they use. They use those papers to improve their service

592
01:10:21.140 --> 01:10:27.339
Maristella Tapia: right, and and they they they store them as part of their database right that they compare other work against.

593
01:10:27.530 --> 01:10:31.820
Maristella Tapia: so there may be some limited use language.

594
01:10:31.850 --> 01:10:56.750
Brian Beatty: I don't think it would be like they can take it and republish it as if it was their own, and that I don't think they would really do that. That's not that what they want, but they need to be able to use it. Just like all of these other tools we use that we're putting. We're putting information into almost all of them say we can use that data, however, we want, and we don't have to even tell you how we're using it, and we don't have to give you the results of whatever we're doing it for

595
01:10:57.240 --> 01:11:04.829
Brian Beatty: that. That's that's the that's the the the negative kind of slant for their language. But essentially I believe that's what a lot of it's saying.

596
01:11:05.150 --> 01:11:05.900
Brian Beatty: Go ahead!

597
01:11:06.460 --> 01:11:22.670
Maristella Tapia: And then the last question, i'm sorry I didn't take any notes on it. But we were talking, and we mentioned a lot of the things that have already been brought up. But one thing that we talked about was just the lack of of knowledge around on technology, kind of

598
01:11:23.220 --> 01:11:28.409
Maristella Tapia: terms of service and and the importance of reviewing those things and the lack around

599
01:11:28.480 --> 01:11:34.630
Maristella Tapia: understanding how data is used in privacy. Just a general understanding of

600
01:11:34.640 --> 01:11:54.139
Maristella Tapia: those those things surrounding technology is super important, because when only a small group of people in the institution have access to that, or know that, then there can't really be a robust discussion around vetting technology like there was with Proctorial, you know, just took a few faculty members to like.

601
01:11:54.150 --> 01:12:09.909
Maristella Tapia: It takes students to tell faculty members like this has been my experience of Procter, and it's been really awful. And then for for faculty to kind of dig into that for it to become something that we had control over deciding whether or not we were going to move forward with it as an institution.

602
01:12:10.060 --> 01:12:14.849
Maristella Tapia: So that was one of I would. I'll just add that as something that Hasn't really

603
01:12:14.970 --> 01:12:26.519
Brian Beatty: been talk too much about it's just we may want to, you know, if you're interested in talking about that we could even bring that out in the discussion Forum. Because, you know, the question for me is, Why?

604
01:12:27.090 --> 01:12:28.539
Brian Beatty: Why? Why?

605
01:12:28.560 --> 01:12:43.819
Brian Beatty: You know it's so easy for faculty to use almost any technology that's not costing much or anything, and it's so easy just to click through and agree to everything. They make it easy to do it, especially if they're grabbing data in some way that you have to. They have to

606
01:12:43.930 --> 01:12:51.959
Brian Beatty: have you tell them. Yes, I've read the terms of service, and I agree to them, whatever it happens to be, but they make it so easy that almost nobody reads that stuff.

607
01:12:51.990 --> 01:12:55.740
Brian Beatty: And the question might be, Why, why don't we care about it?

608
01:12:55.900 --> 01:13:08.749
Brian Beatty: And so that would be, I think, something that would be might be interesting to unpack a little bit more in the online discussion, because I think there's some interesting things we could say about that with the having with, you know, the invisibility of the consequences of

609
01:13:08.760 --> 01:13:19.990
Brian Beatty: of you know, bad behavior with data, or so far removed from potential consequences, either in time or that levels of interaction that we don't even know about it kind of like. An there's an economic conversation there.

610
01:13:20.320 --> 01:13:22.510
Brian Beatty: Alright, but that's for another time.

611
01:13:22.650 --> 01:13:24.379
How about Group 3?

612
01:13:24.530 --> 01:13:25.939
Brian Beatty: Take you Group 4.

613
01:13:26.890 --> 01:13:33.500
Dalton Lobo Dias (he/him): Yeah. We have a lot of similar things from the school perspective from from way and I and then

614
01:13:34.110 --> 01:13:39.089
Dalton Lobo Dias (he/him): and then Adam shared from his a tech consulting company.

615
01:13:39.210 --> 01:13:41.400
Dalton Lobo Dias (he/him): Adam, do you want? Do you want to speak to this?

616
01:13:42.470 --> 01:13:54.770
Adam Hill (he/him/his): Yeah, I can just say like, because we are provide. My My company provides a lot of services to other universities to them bring their programs online. So a lot of what we're thinking of is like, Of course, the cost.

617
01:13:54.870 --> 01:14:06.119
Adam Hill (he/him/his): and especially the cost at scale and then like, Can this product scale as another thing? And then privacy is also huge, because a breach of privacy, when we're providing this service to a lot of people could have

618
01:14:06.470 --> 01:14:08.300
Adam Hill (he/him/his): a huge impact on.

619
01:14:08.640 --> 01:14:12.820
Adam Hill (he/him/his): And then another thing that's big is just like Lti compatibility can. This

620
01:14:12.830 --> 01:14:30.869
Adam Hill (he/him/his): is this compatible with all other lms.

621
01:14:32.820 --> 01:14:43.180
Adam Hill (he/him/his): I think one thing that we also were interested in is like this idea of digital equity, and, like are the products ex only accessible to people with high speed Internet

622
01:14:43.250 --> 01:14:55.839
Adam Hill (he/him/his): on. And I think that we we were talking about how, with way in Dalton, how that really came to bear when you're teaching everyone's teaching remotely that some students were able to access zoom easily and others not on.

623
01:14:56.270 --> 01:15:08.040
Adam Hill (he/him/his): And then and then that brought up the question for me. Dr. B. Like will is that is digital equity and access to broadband and kind of those issues around emerging technology. Something we'll be discussing as well

624
01:15:08.250 --> 01:15:09.030
Adam Hill (he/him/his): cool.

625
01:15:10.090 --> 01:15:11.070
Brian Beatty: Bring it up.

626
01:15:11.130 --> 01:15:21.169
Adam Hill (he/him/his): Okay, great. And then, yeah, Dalton, I don't know where where we from there. Yeah, I mean it's what you know. This. This is having a lot of my experience, where

627
01:15:21.910 --> 01:15:31.839
Dalton Lobo Dias (he/him): questions of review only happen when there is a crisis when there's like a big problem, or if there is major stakeholder complaint usually from the parents, or from the board, or something like that.

628
01:15:31.920 --> 01:15:35.509
Dalton Lobo Dias (he/him): But you know, like Dr. Beer was saying, is like, If something is free and

629
01:15:35.830 --> 01:15:46.389
Dalton Lobo Dias (he/him): like we can, we we can get access to it. It's game on like teachers. We use, whatever the kids we use, whatever so, and and and there's no there's no review. I think the review

630
01:15:46.430 --> 01:16:00.039
Dalton Lobo Dias (he/him): maybe it takes place in my context, like if we were about to buy a service, and we would review the the vendors. But I, you know, I think what was some somebody was in. My Stella was saying earlier that, like

631
01:16:00.110 --> 01:16:01.080
Dalton Lobo Dias (he/him): it's

632
01:16:01.170 --> 01:16:09.150
Dalton Lobo Dias (he/him): we're we're teachers that we're kind of removed from this process a bit, I mean. Sometimes it's like a committee, or they will make like some kind of task force, but

633
01:16:09.190 --> 01:16:11.200
Dalton Lobo Dias (he/him): it's not something that we do.

634
01:16:11.600 --> 01:16:22.579
Brian Beatty: It's true, but you have the power essentially to bring technologies in for your students to use, even if even if you know you're supposed to have permission. Sometimes, you know, teachers often can do it without that.

635
01:16:22.600 --> 01:16:28.119
Brian Beatty: It may not. It's not ethical in that situation, but it still happens, it certainly can happen.

636
01:16:29.060 --> 01:16:41.530
Adam Hill (he/him/his): It. It also makes you think about like the ways that other large, large corporate entities and foundations have an impact on this, like my friend works for a company that does computer science education

637
01:16:41.910 --> 01:16:52.989
Adam Hill (he/him/his): that I think they provide the education for free to schools. But it's all funded by for Verizon. So Verizon, once that once like, has an interest in it, and like the way this is that corporate interests

638
01:16:53.070 --> 01:17:01.960
Adam Hill (he/him/his): can have this underlying way of like providing something for free. And so you'll just take it. But what are what's happening underneath that?

639
01:17:02.290 --> 01:17:02.889
Brian Beatty: Yeah.

640
01:17:04.030 --> 01:17:13.059
Brian Beatty: Well, 20 years ago one of the biggest push for the big one of the big push is for for e-learning. The acceptance of e learning as an acceptable way of doing training

641
01:17:13.250 --> 01:17:16.260
Brian Beatty: was from companies like oracle.

642
01:17:16.770 --> 01:17:35.200
Brian Beatty: You know, Larry Ellison used to get on his soap. So yeah, the next killer app is e-learning. Why, well, basically let's dump a whole lot more traffic over the Internet so that you have to kind of upgrade all of your network switches and technologies. And guess who's going to sell those to you? Not Oracle?

643
01:17:35.850 --> 01:17:43.890
Brian Beatty: Who is that working Cisco Cisco. I don't know who I don't know who that. Oh, who's that? Who's Cisco? I don't like whoever it was.

644
01:17:44.250 --> 01:17:50.330
Brian Beatty: Anyway, I thought, Well, yeah, that's that's a little self serving. But I get it. I worked in e learning at the time. So I thought, yeah.

645
01:17:51.600 --> 01:17:54.820
Brian Beatty: let's. Let's do the next killer app e-learning.

646
01:17:56.040 --> 01:17:58.879
Okay, Anything else here

647
01:18:04.000 --> 01:18:05.590
Brian Beatty: around there.

648
01:18:05.810 --> 01:18:07.510
Brian Beatty: Yeah.

649
01:18:08.520 --> 01:18:11.210
Adam Hill (he/him/his): I think most of everything else we thought

650
01:18:11.430 --> 01:18:15.940
Adam Hill (he/him/his): we covered. I think the other thing we're just talking about is like the piloting and pro like

651
01:18:16.160 --> 01:18:24.720
Adam Hill (he/him/his): the piloting and testing process. When you do have control at us at a very local level, even like as a teacher can be

652
01:18:25.070 --> 01:18:32.410
Adam Hill (he/him/his): really cumbersome. And so it's also just like easy. When your district just hand you something or your school to and you something

653
01:18:33.030 --> 01:18:33.800
Brian Beatty: right?

654
01:18:33.920 --> 01:18:46.099
Brian Beatty: Yeah. So if you are in that kind of role, or you ever find yourself in that kind of role, being some one of those people who's checking things out for a larger group of institutions, or even just faculty to use it, could be at a campus level.

655
01:18:46.490 --> 01:18:49.949
Brian Beatty: It could be a at a, you know, a a particular unit in a business.

656
01:18:49.980 --> 01:18:53.599
Brian Beatty: or it could be at a system level. That's a lot of responsibility.

657
01:18:53.670 --> 01:19:13.120
Brian Beatty: And you have you have You have a you have a responsibility, and you might even have some exposure there, legally. Depend upon the situation, you know, because things happen, and sometimes things happen, and there's big consequences. Not, you know. Sometimes they're financial. Sometimes they're more social.

658
01:19:13.340 --> 01:19:15.320
Brian Beatty: right like, you know.

659
01:19:15.510 --> 01:19:23.040
Brian Beatty: the position of a or institution in in the in the world around them in their environment, in their context, in their communities

660
01:19:23.320 --> 01:19:24.600
Brian Beatty: that can have

661
01:19:24.880 --> 01:19:36.729
Brian Beatty: long term ramifications Big it it's it's it's a big deal can be a big deal. So one of the things that I ask you whenever you write a paper in this class and talk about using these technologies, especially the newer technologies

662
01:19:36.740 --> 01:19:52.730
Brian Beatty: is, i'm gonna ask you to, You know, address to some extent at least, these issues of you know, a ethics around or the wrong data, the use of data. The collection of data security costs accessibility. Those kinds of things. You may not know a lot about it, but it is something I want you to at least

663
01:19:52.740 --> 01:20:06.850
Brian Beatty: consider and talk a little bit about it, as you're going to give us an overview of the summary. I think it's an important thing that I just want to reinforce, that it's. It's our responsibility at some level. If we're going to be exposing their students to these.

664
01:20:06.980 --> 01:20:14.459
Brian Beatty: and even if your district is using a tool, they say, oh, yeah, this is fine. No problem, if you know you got concerns about it, and they are legitimate concerns.

665
01:20:14.510 --> 01:20:16.809
Brian Beatty: you know. Maybe you don't use that tool.

666
01:20:17.290 --> 01:20:29.429
Brian Beatty: I mean, there are some tools we use here on campus that I I don't use because I don't like the way I don't like either like I don't need the functionality, or I don't think I need it with with the way I teach. But also maybe i'm not. I'm not so happy about the way it it handles things.

667
01:20:29.980 --> 01:20:30.889
Brian Beatty: So

668
01:20:31.360 --> 01:20:32.250
Brian Beatty: there you go.

669
01:20:32.530 --> 01:20:33.530
Brian Beatty: Let's

670
01:20:34.420 --> 01:20:36.599
Brian Beatty: let's break. Now we'll come back

671
01:20:37.560 --> 01:20:39.960
Brian Beatty: in person in 2 weeks.

672
01:20:40.090 --> 01:20:57.129
Brian Beatty: But online we're we're gonna do the reorganization of the the website, one of the next, either Thursday or Friday. And so by the week. And it should be reorganized a little bit. We'll we'll do the things we talked about. And then next week we'll be doing the AI focus topic all asynchronously. I still think it'll be. I think it'd be very interesting.

673
01:20:57.430 --> 01:20:59.149
Brian Beatty: really looking forward to that.

674
01:20:59.210 --> 01:21:12.590
Brian Beatty: And i'm talking to my colleagues at other institutions, though, about these some of these ideas, and and I'll i'm gonna, you know, if we you know. Maybe i'll let them know that I've been doing this with some grad students, and

675
01:21:12.920 --> 01:21:18.889
Brian Beatty: and I don't think we'll have anything I'm going to share with them, of course, but I give them. Maybe maybe we come up with some great ideas

676
01:21:19.140 --> 01:21:20.899
Brian Beatty: now that we can share in our community.

677
01:21:21.840 --> 01:21:23.360
Brian Beatty: and you can share in your community.

678
01:21:25.020 --> 01:21:28.080
Brian Beatty: you can be on the cutting edge. Oh, yeah, I know that

679
01:21:28.120 --> 01:21:28.969
Brian Beatty: that person.

680
01:21:29.200 --> 01:21:31.889
Brian Beatty: She's always on the cutting edge of this. She knows all about. AI.

681
01:21:32.080 --> 01:21:35.739
Brian Beatty: So there you go. Any any final comments or questions.

682
01:21:38.690 --> 01:21:47.260
Brian Beatty: all right for those of you who listen to the very end. Please participate in our breakout room notes. Add your name to that, and we'll see you online in the forums as well

683
01:21:49.040 --> 01:21:49.920
Brian Beatty: so long.

684
01:21:50.000 --> 01:21:52.400
Brian Beatty: Thank you, Bye, bye.

